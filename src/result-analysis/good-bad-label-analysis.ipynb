{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import spacy\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ground truth and prediction vectors\n",
    "ground_truth = ['F', 'F', 'F', 'T', 'F', 'F', 'F', 'F', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'F', 'T', 'T', 'F', 'T', 'T', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'F', 'T', 'T', 'F', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T', 'F', 'T']\n",
    "prediction = ['T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'T', 'F', 'F', 'F', 'T', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'F', 'T', 'T', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'T', 'F', 'T', 'T', 'F', 'F', 'F', 'F', 'T']\n",
    "\n",
    "# Convert 'T' and 'F' labels to 1 and 0, respectively\n",
    "ground_truth = np.array([1 if label == 'T' else 0 for label in ground_truth])\n",
    "prediction = np.array([1 if label == 'T' else 0 for label in prediction])\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(ground_truth, prediction)\n",
    "\n",
    "# Calculate precision and recall\n",
    "tp = conf_matrix[1, 1]  # True Positives\n",
    "fp = conf_matrix[0, 1]  # False Positives\n",
    "fn = conf_matrix[1, 0]  # False Negatives\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Non-visual VKEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_words = []\n",
    "nonvisual_verb_list_path = \"./../output/cleaned/nonvisual_verb.txt\"\n",
    "with open(nonvisual_verb_list_path, \"r\") as f:\n",
    "    specific_words = f.read().splitlines()\n",
    "\n",
    "# task = \"Video-quality-classification\"\n",
    "# prompt_id = \"1-COT-few-shot-distinguish-visual-quality-classification\" \n",
    "v_list = ['Xn_7rwAHpPs', '8eBU4QfrFL4', 'rss-y2MLjWY', 'Hv0lzJZkfqU', 'XTaBAksP1d8', 'qC83Iem2KzQ', '1-jnsbRQYCM', 'V7xk-kgpZmk', 'X12nWy9hpHs', '_EHNXYrOu0o', '-EsSn8VlAUg', 'qB-E8q3yK4Q', 'ke0YxzcZ1w4', 'L7Qp33o5128', '2nyfmB9choE', 'vFmSrHcm1_0', '0Za89cR8mxg', 'dVPZF7nFafQ', 'OGilPWdaikI', 'ws-uf6IzTdk', '9FSubSIXleo', 'su7G18Flmpg', 'qEceucYcKWM', 'wzYeeEq_bpE', 'ABdevFdahy4', 'RHbDxf4EKb0', 'zfd1ZcS4sXE', 'n5OqxEroivo', 'RBmeiMZQLsA', '5N8WyIVcseM', 'fpgVDVjwUs4', 'xr7kBln3z00', '6vV7-tb6Pxc', 'L5Ehli9oqkY', 'sBeOQl4-jKQ', 'Gl5syT9o2yY', 'fEMqCktQEJo', 'TP0TzvtQSao', '1zpecSpou0Q', 'byPcWUlWiYI', 'jGuN68Z9rAY', 'GqJctGBHPSY', 'SXqqDnZDK0U', 'HiULplVM2nk', 'H9jjNPfeWmA', 'n1MIslYNju8', 'R_DwQGqzNg0', '9JTVO2jnfMM', 'A89U0coZkg4', 'McgjTFyCTfA', 'qTPKGVrFtQU', 'vFgbJomoWac', '8LX4earN0gY', 'T95ZET6snSk', 'obPT0w4LPow', 'pSlCKuSSdkA', 'nFMunlQ4ZrM', 'S1-xgIAVK4s', 'Q_EAYzJFt2g', 'TtA4Txii0us', 'GHYOyr_ISUY', 'eMiuD5ddUaA', '0fUnuFSl4jo', 'oAb2YE-5qnM', 'cdfLK8v5uXs', 'tl0fX2FnYPY', 'weTM5PlAY_Q', 'CuVmueVpJe8', 'NM08R4kWcB8', 'kMsWyPjRUZE', 'SEZp-0O7dkA', 'Avy_bnTPumw', 'CsQ2TJ8RpJE', '59hUv75jAyc', 'IsS_d1FUJRU', 'Ac533ZkHD5I', 'yxO3EONOGf8', 'mfKDaFM2Btk', 's-lSUi9LFlI', 'P5lDJYAsy_E', '4VOOVGdd6Mo', '-yGmk-7X3Fs', 'qDKU0rSnd6k', 'xbSp0TxZpM4', 'aVHdv502hR4', '0kqbw3AwgLo', 'Y6eWmpgvl2k', 'ri4oacr-ZWs', 'F93LW_P3yec', 'Y1D9slLijj0', 'TtTsNNXrRU0', '_CFPQrftMUs', 'ND9g4p7Xmvc', 'ZNSwYGkDtNY', 'xY7kVVS2HVY', 'NQU7dxNJZEs', '3epRsvmx1ws', 'Y0jNEWPsrvw', 'hUuvVrENhJk', '8SUl15KT-UE']\n",
    "\n",
    "output_file_path = f\"./../output/cleaned/1-COT-few-shot-distinguish-visual-quality-classification/\"\n",
    "input_file_path = \"./../output/Video-quality-classification-result/1-COT-few-shot-distinguish-visual-quality-classification/\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Verbs from VKEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = set()\n",
    "i = 0\n",
    "verb_list_path = \"./../output/cleaned/verb.txt\"\n",
    "for vid in v_list:\n",
    "    output_file_path = f\"./../output/cleaned/1-COT-few-shot-distinguish-visual-quality-classification/{vid}.txt\"\n",
    "    input_file_path = f\"./../output/Video-quality-classification-result/1-COT-few-shot-distinguish-visual-quality-classification/{vid}.txt\"\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        with open(input_file_path, \"r\") as input_file:\n",
    "            # Loop through each line in the file\n",
    "            for line in input_file:\n",
    "                words = line.split()\n",
    "                if (len(words) > 3) and (words[0] == \"Visual\") and (words[1] == \"Key\") and (words[2] == \"Event\"):\n",
    "\n",
    "                    nlp_line = nlp(line)\n",
    "                    verbs = [token.text for token in nlp_line if token.pos_ == \"VERB\"]\n",
    "                    if len(verbs) != 0:\n",
    "                        verb_list.add(verbs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(verb_list_path, \"w\") as verb_list_file:\n",
    "    for verb in verb_list:\n",
    "        verb_list_file.write(verb + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter VKEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid in v_list:\n",
    "    output_file_path = f\"./../output/cleaned/1-COT-few-shot-distinguish-visual-quality-classification/{vid}.txt\"\n",
    "    input_file_path = f\"./../output/Video-quality-classification-result/1-COT-few-shot-distinguish-visual-quality-classification/{vid}.txt\"\n",
    "    # Open the output file in write mode\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        with open(input_file_path, \"r\") as input_file:\n",
    "            # Loop through each line in the file\n",
    "            for line in input_file:\n",
    "                words = line.split()\n",
    "                if (len(words) > 3) and (words[0] == \"Visual\") and (words[1] == \"Key\") and (words[2] == \"Event\"):\n",
    "                    isVisual = True\n",
    "                    if words[3].endswith(\"(non-visual):\"):\n",
    "                        isVisual = False\n",
    "                        continue\n",
    "                    for word in specific_words:\n",
    "                        if word in words:\n",
    "                            isVisual = False\n",
    "                            continue\n",
    "                    if isVisual:\n",
    "                        output_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['concludes', 'conclude', 'joke', 'described', 'instructs', 'shares', 'introduce', 'decides', 'decide', 'evaluates', 'reiterates', 'assures', 'calling', 'recalls', 'introduces', 'acknowledges', 'praised', 'repeats', 'reflect', 'recounts', 'reviews', 'summarizes', 'confirms', 'speaks', 'highlight', 'reminisces', 'announces', 'discussing', 'welcomes', 'accused', 'admires', 'discuss', 'explains', 'struggles', 'praises', 'interviews', 'asks', 'introduced', 'admired', 'plans', 'reflects', 'hoping', 'advises', 'immerses', 'refers', 'reminds', 'questions', 'mention', 'emphasizes', 'talks', 'suggests', 'express', 'mentions', 'encourages', 'discusses', 'calls', 'expresses', 'interrupts', 'describes', 'says', 'reminded', 'quotes']\n"
     ]
    }
   ],
   "source": [
    "print(specific_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output VKE to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the directory containing the text files\n",
    "directory_path = \"./../output/Video-quality-classification-result/1-COT-few-shot-distinguish-visual-quality-classification\"\n",
    "\n",
    "# Set the path to the output CSV file\n",
    "output_file_path = \"./../output/cleaned/output.csv\"\n",
    "\n",
    "# Loop through each file in the directory\n",
    "with open(output_file_path, \"w\", newline='') as output_file:\n",
    "    csv_writer = csv.writer(output_file)\n",
    "    csv_writer.writerow([\"vid\", \"VKE lsit\", \"GPT isVisual Label\", \"GPT + Keywords isVisual Label\"])\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\") as input_file:\n",
    "                # Write the filename to the first row of the CSV file\n",
    "                csv_writer.writerow([filename])\n",
    "                # Loop through each line in the file and write it to the CSV file\n",
    "                for line in input_file:\n",
    "                    words = line.split()\n",
    "\n",
    "                    if (len(words) > 3) and (words[0] == \"Visual\") and (words[1] == \"Key\") and (words[2] == \"Event\"):\n",
    "                        is_visual = True\n",
    "                        is_GPT_visual = True\n",
    "\n",
    "                        nlp_line = nlp(line)\n",
    "                        verbs = [token.text for token in nlp_line if token.pos_ == \"VERB\"]\n",
    "\n",
    "                        if(words[3].endswith(\"(non-visual):\")):\n",
    "                            is_GPT_visual = False\n",
    "                            is_visual = False\n",
    "                        if len(verbs) != 0:\n",
    "                            if verbs[0] in specific_words:\n",
    "                                is_visual = False\n",
    "                        if is_visual:\n",
    "                            if is_GPT_visual:\n",
    "                                csv_writer.writerow([\"\", line.strip(), \"T\", \"T\"])\n",
    "                            else:\n",
    "                                csv_writer.writerow([\"\", line.strip(), \"F\", \"T\"])\n",
    "                        else:\n",
    "                            if is_GPT_visual:\n",
    "                                csv_writer.writerow([\"\", line.strip(), \"T\", \"F\"])\n",
    "                            else:\n",
    "                                csv_writer.writerow([\"\", line.strip(), \"F\", \"F\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT filtering confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../output/cleaned/output_with_ground_truth.csv')\n",
    "\n",
    "raw_ground_truth = []\n",
    "raw_gpt_prediction = []\n",
    "raw_gpt_keywords_prediction = []\n",
    "raw_visual_nonvisual = []\n",
    "for gt, gpt_label, gpt_keywords_label, visual_nonvisual in zip(df[\"Ground Truth\"], df[\"GPT isVisual Label\"], df[\"GPT + Keywords isVisual Label\"], df[\"Is Visual  + Non-visual\"]):\n",
    "    if gt in [\"T\", \"F\"]:\n",
    "        raw_ground_truth.append(gt)\n",
    "    if gpt_label in [\"T\", \"F\"]:\n",
    "        raw_gpt_prediction.append(gpt_label)\n",
    "    if gpt_keywords_label in [\"T\", \"F\"]:\n",
    "        raw_gpt_keywords_prediction.append(gpt_keywords_label)\n",
    "    if visual_nonvisual in [\"T\", \"F\"]:\n",
    "        raw_visual_nonvisual.append(visual_nonvisual)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "gpt_keywords_matrix = confusion_matrix(raw_ground_truth, raw_gpt_keywords_prediction)\n",
    "gpt_matrix = confusion_matrix(raw_ground_truth, raw_gpt_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "GPT False Negatives: 48\n",
      "\n",
      "\n",
      "2:\n",
      "Percentage of visual + non-visual: 0.2901353965183752 ( 150 / 517 )\n",
      "\n",
      "\n",
      "4:\n",
      "[[754  39]\n",
      " [ 61 456]]\n",
      "\n",
      "\n",
      "5:\n",
      "[[630 163]\n",
      " [ 48 469]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1\n",
    "print(\"1:\")\n",
    "gpt_fn = gpt_matrix[1, 0]  # actual: T, predicted: F\n",
    "print(\"GPT False Negatives:\", gpt_fn)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2\n",
    "print(\"2:\")\n",
    "visual_event_num = raw_ground_truth.count(\"T\")\n",
    "visual_nonvisual_num = raw_visual_nonvisual.count(\"T\")\n",
    "precentage = visual_nonvisual_num / visual_event_num\n",
    "print(\"Percentage of visual + non-visual:\", precentage, \"(\", visual_nonvisual_num, \"/\", visual_event_num, \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4\n",
    "print(\"4:\")\n",
    "print(gpt_keywords_matrix)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 5\n",
    "print(\"5:\")\n",
    "print(gpt_matrix)\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7944514501891551\n",
      "Recall: 0.9292035398230089\n"
     ]
    }
   ],
   "source": [
    "tp = gpt_matrix[0, 0]  # True Positives\n",
    "fp = gpt_matrix[0, 1]  # False Positives\n",
    "fn = gpt_matrix[1, 0]  # False Negatives\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754 39 61\n",
      "Precision: 0.9508196721311475\n",
      "Recall: 0.9251533742331288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tp = gpt_keywords_matrix[0, 0]  # True Positives\n",
    "fp = gpt_keywords_matrix[0, 1]  # False Positives\n",
    "fn = gpt_keywords_matrix[1, 0]  # False Negatives\n",
    "\n",
    "print(tp, fp, fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_visual_nonvisual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
